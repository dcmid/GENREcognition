{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Recognition - Milestone 1\n",
    "Darren Midkiff and Cheng-Wei Hu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This project aims to identify the genre of music in an audio sample. Features will be extracted from the analog data, and a genre will be predicted using a KNN model, trained on labelled audio samples from the freely available FMA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "The [FMA Dataset](https://github.com/mdeff/fma) is comprised of over 100,000 tracks from 161 genres. In order to make the problem more manageable, we will use the small version of the dataset, which includes 8,000 tracks from 8 top-level genres. The dataset also includes dozens of features -- year released, location of artist, number of listens, etc. Because this project aims to identify genre using only audio signal, all of these features are irrelevant and will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>genre_top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140</td>\n",
       "      <td>Folk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>Folk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track_id genre_top\n",
       "0        2   Hip-Hop\n",
       "1        5   Hip-Hop\n",
       "2       10       Pop\n",
       "3      140      Folk\n",
       "4      141      Folk"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read full metadata file\n",
    "metadata = pd.read_csv(\"./fma_metadata/tracks.csv\", skiprows=[0,2], low_memory=False)\n",
    "\n",
    "# drop all tracks that are not in fma_small dataset\n",
    "metadata = metadata[metadata[\"subset\"].eq(\"small\")]\n",
    "# add name to track_id column (missing because of stupid CSV formatting)\n",
    "metadata = metadata.rename(columns={\"Unnamed: 0\": \"track_id\"})\n",
    "# drop all columns that don't relate to genre\n",
    "# we will not have this metadata from the audio file\n",
    "metadata.drop(metadata.columns.difference([\"track_id\",\"genre_top\"]),1,inplace=True)\n",
    "# reset indices accounting for dropped rows\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "#write only relevant metadata to file for use in training\n",
    "metadata.to_csv(\"fma_small_genres.csv\")\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hip-Hop' 'Pop' 'Folk' 'Experimental' 'Rock' 'International' 'Electronic'\n",
      " 'Instrumental']\n"
     ]
    }
   ],
   "source": [
    "# show 8 genres\n",
    "print(metadata[\"genre_top\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply machine learning techniques to audio samples, useful features must be extracted from the signals. We will extract four features: [Zero Crossing Rate](https://en.wikipedia.org/wiki/Zero-crossing_rate), [Spectral Centroid](https://en.wikipedia.org/wiki/Spectral_centroid), [Spectral Rolloff](https://en.wikipedia.org/wiki/Spectral_slope), and [Mel-Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum).\n",
    "\n",
    "\n",
    "In our project, we will use [librosa](https://librosa.org/doc/latest/index.html) to extract features from raw audio. Here, features are extracted from our training data and exported to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the audio file paths from the fma_small dataset \n",
    "import os\n",
    "file_names = []\n",
    "for root, dirs, files in os.walk('./fma_small', topdown=False):\n",
    "    for name in files:\n",
    "        if name[-1] != '3':\n",
    "            continue\n",
    "        file_names.append(os.path.join(root, name))\n",
    "# print(len(file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract the four features from  \n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "\n",
    "def extract_feature_from_audio(audio_path, should_plot = False, should_print = False):\n",
    "    # load\n",
    "    x , sr = librosa.load(audio_path)\n",
    "    if should_plot:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.waveplot(x, sr=sr)\n",
    "    \n",
    "    # zero_crossing_rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(x)\n",
    "    m_zcr = np.mean(zero_crossing_rate) #mean zero-crossing rate\n",
    "    v_zcr = np.var(zero_crossing_rate) #zero-crossing rate variance\n",
    "    \n",
    "    if should_print:\n",
    "        print(zero_crossing_rate.shape)\n",
    "        print(zero_crossing_rate)\n",
    "        print(m_zcr)\n",
    "    \n",
    "    # spectral_centroids\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
    "    m_spec_cent = np.mean(spec_cent) #mean spectral centroid\n",
    "    v_spec_cent = np.var(spec_cent) #spectral centroid variance\n",
    "    \n",
    "    if should_print:\n",
    "        print(spectral_centroids.shape)\n",
    "        print(spectral_centroids)\n",
    "        print(m_spec_cent)\n",
    "\n",
    "    # Computing the time variable for visualization\n",
    "    # frames = range(len(spectral_centroids))\n",
    "    # t = librosa.frames_to_time(frames)\n",
    "    \n",
    "    # spectral_rolloff\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(x, sr=sr)[0]\n",
    "    m_spec_roll = np.mean(spectral_rolloff) #mean spectral rolloff\n",
    "    v_spec_roll = np.var(spectral_rolloff) #spectral rolloff variance\n",
    "    \n",
    "    if should_print:\n",
    "        print(spectral_rolloff.shape)\n",
    "        print(spectral_rolloff)\n",
    "        print(m_spec_roll)\n",
    "\n",
    "    # mfccs\n",
    "    mfccs = librosa.feature.mfcc(x, sr=sr)\n",
    "    m_mffcs = np.mean(mffcs,axis=1) #mean mfccs\n",
    "    v_mfccs = np.var(mfccs,axis=1) #mfcc variances\n",
    "    \n",
    "    if should_print:\n",
    "        print(mfccs.shape)\n",
    "        print(mfccs)\n",
    "    #Displaying  the MFCCs:\n",
    "    # librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    \n",
    "    return [m_zcr, v_zcr, m_spec_cent, v_spec_cent, m_spec_roll, v_spec_roll, m_mfccs, v_mfccs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,sr = librosa.load(\"./fma_small/000/000002.mp3\")\n",
    "zero_crossing_rate = librosa.feature.zero_crossing_rate(x)[0]\n",
    "spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(x, sr=sr)[0]\n",
    "mfccs = librosa.feature.mfcc(x, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1291,)\n",
      "(1291,)\n",
      "(1291,)\n",
      "(20, 1291)\n"
     ]
    }
   ],
   "source": [
    "print(zero_crossing_rate.shape)\n",
    "print(spectral_centroids.shape)\n",
    "print(spectral_rolloff.shape)\n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, sr = librosa.load(file_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./fma_small\\\\000\\\\000002.mp3'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./fma_small\\000\\000002.mp3\n",
      "Failed:  0 ./fma_small\\000\\000002.mp3\n",
      "1 ./fma_small\\000\\000005.mp3\n",
      "Failed:  1 ./fma_small\\000\\000005.mp3\n",
      "2 ./fma_small\\000\\000010.mp3\n",
      "Failed:  2 ./fma_small\\000\\000010.mp3\n",
      "3 ./fma_small\\000\\000140.mp3\n",
      "Failed:  3 ./fma_small\\000\\000140.mp3\n",
      "4 ./fma_small\\000\\000141.mp3\n",
      "Failed:  4 ./fma_small\\000\\000141.mp3\n"
     ]
    }
   ],
   "source": [
    "# Extract the features from audio files\n",
    "# Some of the audio files are damaged. So we skipped those files.\n",
    "# The damaged files are: './fma_small/099/099134.mp3', './fma_small/108/108925.mp3', './fma_small/133/133297.mp3'\n",
    "\n",
    "train_audio_features_all = []\n",
    "fail_file_names_all = []\n",
    "fail_file_idx_all = []\n",
    "fail_file_names_dict_all = {}\n",
    "\n",
    "for idx, file in enumerate(file_names[:5]):\n",
    "    print(idx, file)\n",
    "    try:\n",
    "        single_audio_features = extract_feature_from_audio(file)\n",
    "#         row = []\n",
    "#         row.append(file.split('/')[-1])\n",
    "#         for f in single_audio_features:\n",
    "#             row.append(f)\n",
    "#         train_audio_features_all.append(row)\n",
    "    except:\n",
    "        print(\"Failed: \", idx, file)\n",
    "        fail_file_names_all.append(file)\n",
    "        fail_file_idx_all.append(file)\n",
    "        fail_file_names_dict_all[file] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_audio_features_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bcf5b02bf2da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_audio_features_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_audio_features_all' is not defined"
     ]
    }
   ],
   "source": [
    "len(train_audio_features_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fma_small\\000\\000002.mp3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-15f9acb3b314>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int32' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Export the features into a csv files\n",
    "import csv\n",
    "\n",
    "with open('features_all.csv', mode='w') as features_file:\n",
    "    features_file = csv.writer(features_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    features_names = [\"file_name\", \"zero_crossing (zero_crossing_sum & total_frame)\", \"spectral_centroids\", \"spectral_rolloff\", \"mfccs\"]\n",
    "    features_file.writerow(features_names)\n",
    "    for idx, raw_feature in enumerate(train_audio_features_all):\n",
    "        row = []\n",
    "        for i, f in enumerate(raw_feature):\n",
    "            if i == 0:\n",
    "                row.append(f)\n",
    "                print(idx, f)\n",
    "            elif i == 1:\n",
    "                row.append([sum(f), len(f)])\n",
    "            else:\n",
    "                row.append(f.tolist())\n",
    "        features_file.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exported csv file can be found [here](https://drive.google.com/file/d/1xiyYZ23WVl-RyWIWlf3-JZaZll4i9b1X/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the model training easier, we have to preprocess the data and combine the `features_all.csv` files with `fma_small_genres.csv`, storing all calculated features with their respective track IDs. This will be extremely helpful in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read features_all.csv and process the file_name columns\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"./features_all.csv\", low_memory=False)\n",
    "raw_data = raw_data.rename(columns={\"file_name\": \"track_id\"})\n",
    "raw_data['track_id'] = raw_data['track_id'].str[:-4].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read features_all.csv and make it a dictionary\n",
    " and process the file_name columns\n",
    "\n",
    "metadata = pd.read_csv(\"./fma_small_genres.csv\", low_memory=False)\n",
    "metadata_mapping = dict([(i,g) for i, g in zip(metadata.track_id, metadata.genre_top)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a column genre_top into raw_data with each track's corresponding genre_top\n",
    "raw_data[\"genre_top\"] = raw_data[\"track_id\"].map(metadata_mapping)\n",
    "raw_data.to_csv(\"fma_small_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exported csv file can be found [here](https://drive.google.com/file/d/1hTANS4oluY0VlIYyS-fjamg4p1zPpsT1/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now that we have labeled training data and corresponding features, the only remaining step is to train a model. We are strongly considering a nearest-neighbors model, but the size of the training dataset may prove prohibitive in this pursuit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
